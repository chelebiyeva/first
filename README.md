# KNN (K-nearest neighbors)
**Задача классификации в машинном обучении** — это задача отнесения объекта к одному из заранее определенных классов на основании его формализованных признаков. Каждый из объектов в этой задаче представляется в виде вектора в N-мерном пространстве, каждое измерение в котором представляет собой описание одного из признаков объекта.Для обучения классификатора необходимо иметь набор объектов, для которых заранее определены классы. Это множество называется обучающей выборкой, её разметка производится вручную, с привлечением специалистов в исследуемой области. 

**Алгоритм**

Для классификации каждого из объектов тестовой выборки необходимо последовательно выполнить следующие операции:

 - 	Вычислить расстояние до каждого из объектов обучающей выборки
 -  Отобрать k объектов обучающей выборки, расстояние до которых минимально
 -	Класс классифицируемого объекта — это класс, наиболее часто встречающийся среди k ближайших соседей
  
**Метод ближайших соседей** — простейший метрический классификатор, основанный на оценивании сходства объектов. Классифицируемый объект относится к тому классу, которому принадлежат ближайшие к нему объекты обучающей выборки.

Для любого объекта **u∈Х** расположим элементы обучающей выборки **x1,x2,...,x** по мере возрастания расстояния до **u** Метрический алгоритм классификации с обучающей выборкой **X** относит объект u к тому классу **y∈Y** , для которого суммарный вес ближайших обучающих объектов максимален:

![Иллюстрация к проекту](https://github.com/chelebiyeva/first/raw/master/knn.png)


 
 где весовая функция **ω(i, u)** оценивает степень важности i-го соседа для классификации объекта **u**. Эта функция неотрицательна и не возрастает по i.
Если по-разному задавать весовую функцию, можно получать различные варианты метода ближайших соседей.
- **ω(i, u) = [i = 1]** — простейший метод ближайшего соседа;
- **ω(i, u) = [i =< k]** — метод k ближайших соседей;
- **ω(i, u) = [i =< k]qi** — метод k экспоненциально взвешенных ближайших соседей, где предполагается q < 1;
Рассмотрим KNN для i =< k



Рассмотрим KNN для i =< k

Пример выборки IrisFishers 
![Иллюстрация к проекту](https://github.com/chelebiyeva/first/raw/master/knn1.png)

**Преимущества:**

- Простота реализации.
- При k, подобранном около оптимального, алгоритм "неплохо" классифицирует.

**Недостатки:**

- Нужно хранить всю выборку.
- При k = 1 неустойчивость к погрешностям (выбросам -- объектам, которые окружены объектами чужого класса), вследствие чего этот выброс классифицировался неверно и окружающие его объекты, для которого он окажется ближайшим, тоже.
- При k = l алгоритм наоборот чрезмерно устойчив и вырождается в константу.
- Максимальная сумма объектов в counts может достигаться в нескольких классах одновременно.
- "Скудный" набор параметров.
- Точки, расстояние между которыми одинаково, не все будут учитываться.

# «Наивный» байесовский классификатор

Теорема Байеса позволяет переставить местами причину и следствие. Зная с какой вероятностью причина приводит к некоему событию, эта теорема позволяет расчитать вероятность того что именно эта причина привела к наблюдаемому событию.

**Теорема**
![Иллюстрация к проекту](https://github.com/chelebiyeva/first/raw/master/fb.png)
Для событий A и B, при условии, что P(B) ≠ 0,


Цель классификации состоит в том чтобы понять к какому классу принадлежит элемент, поэтому нам нужна не сама вероятность, а наиболее вероятный класс. Байесовский классификатор использует оценку апостериорного максимума (Maximum a posteriori estimation) для определения наиболее вероятного класса. Грубо говоря, это класс с максимальной вероятностью. То есть нам надо рассчитать вероятность для всех классов и выбрать тот класс, который обладает максимальной вероятностью.

Формула рассчета:

alt text
