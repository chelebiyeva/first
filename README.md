# KNN (K-nearest neighbors)
**Задача классификации в машинном обучении** — это задача отнесения объекта к одному из заранее определенных классов на основании его формализованных признаков. Каждый из объектов в этой задаче представляется в виде вектора в N-мерном пространстве, каждое измерение в котором представляет собой описание одного из признаков объекта.Для обучения классификатора необходимо иметь набор объектов, для которых заранее определены классы. Это множество называется обучающей выборкой, её разметка производится вручную, с привлечением специалистов в исследуемой области. 

**Алгоритм**

Для классификации каждого из объектов тестовой выборки необходимо последовательно выполнить следующие операции:

 - 	Вычислить расстояние до каждого из объектов обучающей выборки
 -  Отобрать k объектов обучающей выборки, расстояние до которых минимально
 -	Класс классифицируемого объекта — это класс, наиболее часто встречающийся среди k ближайших соседей
  
**Метод ближайших соседей** — простейший метрический классификатор, основанный на оценивании сходства объектов. Классифицируемый объект относится к тому классу, которому принадлежат ближайшие к нему объекты обучающей выборки.

Для любого объекта **u∈Х** расположим элементы обучающей выборки **x1,x2,...,x** по мере возрастания расстояния до **u** Метрический алгоритм классификации с обучающей выборкой **X** относит объект u к тому классу **y∈Y** , для которого суммарный вес ближайших обучающих объектов максимален:

![Иллюстрация к проекту](https://github.com/chelebiyeva/first/raw/master/knn.png)


 
 где весовая функция **ω(i, u)** оценивает степень важности i-го соседа для классификации объекта **u**. Эта функция неотрицательна и не возрастает по i.
Если по-разному задавать весовую функцию, можно получать различные варианты метода ближайших соседей.
- **ω(i, u) = [i = 1]** — простейший метод ближайшего соседа;
- **ω(i, u) = [i =< k]** — метод k ближайших соседей;
- **ω(i, u) = [i =< k]qi** — метод k экспоненциально взвешенных ближайших соседей, где предполагается q < 1;
Рассмотрим KNN для i =< k

**Преимущества:**

-Простота реализации.
-При k, подобранном около оптимального, алгоритм "неплохо" классифицирует.

**Недостатки:**

-Нужно хранить всю выборку.
-При k = 1 неустойчивость к погрешностям (выбросам -- объектам, которые окружены объектами чужого класса), вследствие чего этот выброс классифицировался неверно и окружающие его объекты, для которого он окажется ближайшим, тоже.
-При k = l алгоритм наоборот чрезмерно устойчив и вырождается в константу.
-Максимальная сумма объектов в counts может достигаться в нескольких классах одновременно.
-"Скудный" набор параметров.
-Точки, расстояние между которыми одинаково, не все будут учитываться.

Рассмотрим KNN для i =< k

Пример выборки IrisFishers 
![Иллюстрация к проекту](https://github.com/chelebiyeva/first/raw/master/knn1.png)
